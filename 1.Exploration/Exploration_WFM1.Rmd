---
title: "Exploration_WFM1"
author: "Milton Osiel Candela Leal"
date: "7/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(R.matlab)
library(dplyr)
library(caret)
library(MASS)
```

## Packages used
The following packages were used:
```{r lib, eval = FALSE}
library(R.matlab)
library(dplyr)
library(caret)
library(MASS)
```

## Reading information
The information is on **.mat** files, and so it must be transformed into a Data Frames via **R.matlab** and **dplyr** packages.

```{r read}
data <- readMat('Data/Raw/FeatureMatrices.mat')

unmin <- as.data.frame(data$WFM.1) # Using the first document of 1 minute

feature_names <- unlist(data$FMinfo) # Get column names
colnames(unmin) <- feature_names

unmin <- as.data.frame(unmin)
unmin <- mutate(unmin, Clas = as.factor(Clas)) # Transform the predicted variable to a factor

unmin <- unmin[,c(1:120,length(colnames(unmin)))] # Remove Empatica's variables (EDA TEMP HR HF)

print(head(unmin))
```

## Information partitioning
We further use the **caret** package in order to divide the whole dataset into a separate training and testing dataset, a seed is used to test with the same division of data.

```{r part}
set.seed(1002)

trainIndex <- createDataPartition(unmin$Clas, p = 0.75, list = FALSE)
training <- unmin[trainIndex,]
testing <- unmin[-trainIndex,]
```

## ML models fitting
Data is already normalized, so it does not require pre-processing and can be directly used by ML algorithms. The following function tests a generated model on training and testing data, the parameters are: the ML algorithm and *k*, the number of cross-validations used.

A further for loop iterate three algorithms (rf, gmb, lda) with an incremental number of cross-validations, from 2 to **n_max**.

```{r prob}
prob_modelo <- function(training, met, testing, control){
     if(met == 'gbm'){
          modelo <- train(Clas ~ ., data = training, method = met,
                          trControl = control, verbose = FALSE)
     } else{
          modelo <- train(Clas ~ ., data = training, method = met,
                          trControl = control)}
     Pred <- predict(modelo, testing)
     acc <- round(confusionMatrix(testing$Clas, Pred)$overall[1], 5)
     return(acc)
}

n_max <- 10 # Maximum number of cross-validations (CVs)
metodos <- c('rf', 'gbm', 'lda')
df <- data.frame('rf' = 2:n_max, 'gbm' = 2:n_max, 'lda' = 2:n_max)

for(metodo in metodos){
        acc_l <- c()
        for(num in 2:n_max){
                control <- trainControl(method = 'cv', number = num)
                acc <- prob_modelo(training, metodo, testing, control)
                acc_l <- c(acc_l, acc) }
        df[,metodo] <- acc_l
}

plot(y = df[,1], x = 2:n_max, ylim = c(0.8, 1), xlim = c(2,10), type = 'l', col = 1, lwd = 2,
     ylab = 'Accuracy', xlab = 'k',
     main = 'Accuracy of algorithms with respect to cross-validations')
lines(y = df[,2], x = 2:n_max, col = 2, lwd = 2)
lines(y = df[,3], x = 2:n_max, col = 3, lwd = 2)
legend('bottomright', legend = c('rf', 'gbm', 'lda'), col = 1:3, lty = 1, lwd = 2)
```

As it can be seen on the plot, the LDA (Linear Discriminant Analysis) model outperformed *rf* and *gbm*, it is also not affected by the number of CVs, so it will be further analyzed.

```{r mas}
modelo_lda <- lda(Clas ~ ., data = training)
```

## Model evaluation

In order to visualize the LDAs predictions, we obtain the two most relevant components (LDA1 and LDA), create a plot of the transformed values and its predicted class on training data.
```{r }
valores <- predict(modelo_lda)

plot(valores$x[,1], valores$x[,2], col = training$Clas, xlab = 'LDA1', ylab = 'LDA2',
     main = 'Clusters of each class')
legend('topright', legend = c(1:3), col = c('black', 'red', 'green'), pch = 20)
```

Using the current weights, the plot on testing data is affected due to an outlier, and thus we compute a confusion matrix on testing data:

```{r }
pred <- predict(modelo_lda, testing)
print(confusionMatrix(testing$Clas, pred$class))
```

The outlier is then padded with the median of the predictions and thus the plot can be visualized.
```{r}
outliers <- pred$x[,1] > 1000
pred$x[outliers,] <- c(median(pred$x[,1]), median(pred$x[,2]))

plot(pred$x[,1], pred$x[,2], col = testing$Clas, xlab = 'LDA1', ylab = 'LDA2',
     main = 'Clusters of each class')
legend('topright', legend = c(1:3), col = c('black', 'red', 'green'), pch = 20)
```

## Plotting the outlier

A visualization of the outlier is presented via an histogram of the value of its features.
```{r }
out_df <- testing[outliers,]
barplot(as.matrix(out_df), las = 2, ylab = 'Standarized value', xlab = 'Feature name',
        cex.names = 0.5, main = paste('Value of row number', row.names(out_df)))
```